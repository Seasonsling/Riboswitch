{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report, roc_curve, auc, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.tree import export_graphviz #plot tree\n",
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from sklearn.metrics import confusion_matrix #for model evaluation\n",
    "import eli5 #for purmutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import shap #for SHAP values\n",
    "from IPython.display import display, HTML\n",
    "from pdpbox import pdp, info_plots #for partial plots\n",
    "import joblib, os\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro',\n",
    "           'rec_macro': 'recall_macro',\n",
    "           'f1_macro':'f1_macro'}\n",
    "\n",
    "\n",
    "title = '156selected_156_1130'\n",
    "KFOLD = 10\n",
    "N_JOBS = 10\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    path = path.strip()\n",
    "    path = path.rstrip(\"\\\\\")\n",
    "    isExists = os.path.exists(path)\n",
    "\n",
    "    if not isExists:\n",
    "        os.makedirs(path)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def preprocess(countpath, testsize = 0.2, randomstate = 1):\n",
    "    x = pd.read_csv('%s' % countpath, index_col=0).T\n",
    "\n",
    "    # get feature names\n",
    "    featurenames = np.array(x.index)\n",
    "\n",
    "    # get label names\n",
    "    x = x.T\n",
    "    y = list(np.array(x.index))\n",
    "    label = sorted(list(set(y)), key = y.index)\n",
    "    \n",
    "    df = x\n",
    "    df['target'] = y\n",
    "\n",
    "    for classname in label:\n",
    "        count_temp = len(df[df.target == classname])\n",
    "        print(\"Proportion of family {0}: {1:.2f}%\".format(classname, (count_temp / (len(df.target)) * 100)))\n",
    "\n",
    "\n",
    "    y = df.target.values\n",
    "    x= df.drop(['target'], axis=1)\n",
    "    \n",
    "    x = x.dropna(axis=1, how='any')  # drop all rows that have any NaN value\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = testsize, random_state = randomstate)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, featurenames, label, df\n",
    "\n",
    "\n",
    "def counting(label, y_train):\n",
    "    counting = {}\n",
    "    for i in label:\n",
    "        count = 0\n",
    "        for j in y_train:\n",
    "            if i == j:\n",
    "                count = count+1        \n",
    "        counting[i] = count\n",
    "    print(counting)\n",
    "    \n",
    "    return counting\n",
    "\n",
    "\n",
    "def smote_dict(label, counting):\n",
    "    dict = {}\n",
    "    for i in label:\n",
    "        if i == 'RF00168':\n",
    "            dict[i] = counting[i] * 6\n",
    "        elif counting[i] > 1000:\n",
    "            dict[i] = counting[i]\n",
    "        elif counting[i] > 700:\n",
    "            dict[i] = counting[i] * 2\n",
    "        elif counting[i] > 500:\n",
    "            dict[i] = counting[i] * 3\n",
    "        elif counting[i] > 200:\n",
    "            dict[i] = counting[i] * 5\n",
    "        else:\n",
    "            dict[i] = counting[i] * 10\n",
    "    print(dict)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def calcu_metrix(scores):\n",
    "    return (np.mean(scores['test_f1_macro']) * 0.6 + np.mean(scores['test_prec_macro'])*0.2 + np.mean(scores['test_rec_macro'])* 0.2)\n",
    "\n",
    "\n",
    "def print_report(name, model, x_test, y_test):\n",
    "    \n",
    "    model_pred = model.predict(x_test)\n",
    "    \n",
    "    # f1 score\n",
    "    print(\"f1 score of {0}: {1:.3f}\".format(name, f1_score(y_test,model_pred, average = 'macro')))\n",
    "    # 模型评估报告\n",
    "    report = classification_report(y_test, model_pred, target_names= label, output_dict=True)\n",
    "    \n",
    "#     dataframe = pd.DataFrame(report).transpose()\n",
    "#     dataframe.to_csv('./Prediction_output_%s/Classification_report/%s_%s.csv' % (title, title, name), index = False)\n",
    "\n",
    "    print(\"Classification report of {0}: \\n{1}\".format(name, classification_report(y_test, model_pred,\n",
    "                                            target_names= label, digits = 3)))\n",
    "\n",
    "    \n",
    "def addtwodimdict(dict, key_a, key_b, val): \n",
    "    if key_a in adic:\n",
    "        dict[key_a].update({key_b: val})\n",
    "    else:\n",
    "        dict.update({key_a:{key_b: val}})\n",
    "\n",
    "\n",
    "def save(model, model_name):\n",
    "    joblib.dump(model, './Model/%s_%s.pkl' % (model_name, title))\n",
    "\n",
    "    \n",
    "def load(model_name):\n",
    "    return joblib.load('./Model/%s_%s.pkl' % (model_name, title))\n",
    "\n",
    "\n",
    "def export_con_matrix(model, model_name):\n",
    "    pred = model.predict(x_test)\n",
    "    confusion = pd.DataFrame(confusion_matrix(y_test, pred))\n",
    "    mkdir('./Confusion_matrix/%s' % title)\n",
    "    confusion.to_csv('./Confusion_matrix/%s/Confusion_matrix_%s_%s_%s.csv' % (title, model_name[0],model_name[1], title))\n",
    "    return confusion\n",
    "\n",
    "\n",
    "models_transfer = {\n",
    "    'knn':'KNN',\n",
    "    'knn_im':'KNN',\n",
    "    'svm':'SVM',\n",
    "    'svm_im':'SVM',\n",
    "    'random':'Random Forest',\n",
    "    'random_im':'Random Forest',\n",
    "    'gb':'Gradient Boosting',\n",
    "    'gb_im':'Gradient Boosting',\n",
    "    'mlp':'MLP',\n",
    "    'mlp_im':'MLP',\n",
    "    'nb':'Naive Bayes',\n",
    "    'nb_im':'Naive Bayes'\n",
    "}\n",
    "\n",
    "\n",
    "models_names = {\n",
    "    'knn':'Balanced',\n",
    "    'knn_im':'Imbalanced',\n",
    "    'svm':'Balanced',\n",
    "    'svm_im':'Imbalanced',\n",
    "    'random':'Balanced',\n",
    "    'random_im':'Imbalanced',\n",
    "    'gb':'Balanced',\n",
    "    'gb_im':'Imbalanced',\n",
    "    'mlp':'Balanced',\n",
    "    'mlp_im':'Imbalanced',\n",
    "    'nb':'Balanced',\n",
    "    'nb_im':'Imbalanced'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model\n",
    "def KNN(x_train, y_train, x_test, y_test, strategy):\n",
    "\n",
    "    # try ro find best k value\n",
    "    best = make_pipeline(SMOTE(random_state = 5, k_neighbors = 5), KNeighborsClassifier(n_neighbors=6))\n",
    "    scores = cross_validate(best, x_train, y_train, scoring = scoring, cv = KFOLD, n_jobs=N_JOBS)\n",
    "    best_scores = scores\n",
    "    print(6)\n",
    "    print(scores)\n",
    "    best_metrix = calcu_metrix(scores)\n",
    "    best.fit(x_train, y_train)\n",
    "\n",
    "    scoreList = []\n",
    "    scoreList.append(best.score(x_test, y_test))\n",
    "    for i in range(8, 16, 2):\n",
    "        knn2 = make_pipeline(SMOTE(random_state = 5, k_neighbors = 5, sampling_strategy = strategy), \\\n",
    "                             KNeighborsClassifier(n_neighbors=i)) # n_neighbors means \n",
    "        scores = cross_validate(knn2, x_train, y_train, scoring = scoring, cv = KFOLD, n_jobs=N_JOBS)\n",
    "        temp_metrix = calcu_metrix(scores)\n",
    "        print(i)\n",
    "        print(scores)\n",
    "        if temp_metrix > best_metrix:\n",
    "            best = knn2\n",
    "            best_scores = scores\n",
    "        knn2.fit(x_train, y_train)\n",
    "        scoreList.append(knn2.score(x_test, y_test))\n",
    "\n",
    "    \n",
    "    plt.plot(range(6, 16, 2), scoreList)\n",
    "    plt.xticks(np.arange(6, 16, 2))\n",
    "    plt.xlabel(\"K value\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(\"./Prediction_output_%s/knn_iteration.png\" % title)\n",
    "    plt.show()\n",
    "    print(\"Maximum KNN Acuracy Score is {:.2f}%\".format((max(scoreList)) * 100))\n",
    "    \n",
    "    \n",
    "    print_report('knn', best, x_test, y_test)\n",
    "    best_scores = pd.DataFrame(best_scores).transpose()\n",
    "    best_scores.to_csv('./Prediction_output_%s/knn_cross_validation_score.csv' % title, index=True)\n",
    "\n",
    "    return best\n",
    "\n",
    "def KNN_im(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    # try ro find best k value\n",
    "    best = KNeighborsClassifier(n_neighbors=6)\n",
    "    scores = cross_validate(best, x_train, y_train, scoring = scoring, cv = KFOLD, n_jobs=N_JOBS)\n",
    "    best_scores = scores\n",
    "    print(6)\n",
    "    print(scores)\n",
    "    best_metrix = calcu_metrix(scores)\n",
    "    best.fit(x_train, y_train)\n",
    "\n",
    "    scoreList = []\n",
    "    scoreList.append(best.score(x_test, y_test))\n",
    "    for i in range(8, 16, 2):\n",
    "        knn2 =KNeighborsClassifier(n_neighbors=i)  # n_neighbors means \n",
    "        scores = cross_validate(knn2, x_train, y_train, scoring = scoring, cv = KFOLD, n_jobs=N_JOBS)\n",
    "        temp_metrix = calcu_metrix(scores)\n",
    "        print(i)\n",
    "        print(scores)\n",
    "        if temp_metrix > best_metrix:\n",
    "            best = knn2\n",
    "            best_scores = scores\n",
    "        knn2.fit(x_train, y_train)\n",
    "        scoreList.append(knn2.score(x_test, y_test))\n",
    "        \n",
    "\n",
    "    plt.plot(range(6, 16, 2), scoreList)\n",
    "    plt.xticks(np.arange(6, 16, 2))\n",
    "    plt.xlabel(\"K value\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(\"./Prediction_output_%s/knn_im_iteration.png\" % title)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Maximum KNN_im Acuracy Score is {:.2f}%\".format((max(scoreList)) * 100))\n",
    "    \n",
    "    print_report('knn_im', best, x_test, y_test)\n",
    "    best_scores = pd.DataFrame(best_scores).transpose()\n",
    "    best_scores.to_csv('./Prediction_output_%s/knn_im_cross_validation_score.csv' % title, index=True)\n",
    "\n",
    " \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(x_train, y_train,x_test, y_test, strategy):\n",
    "    \n",
    "    best = make_pipeline(SMOTE(sampling_strategy = strategy), SVC(cache_size=4096, kernel='linear'))\n",
    "    best_metrix = 0\n",
    "    scoreList = []\n",
    "    for i in ('poly', 'rbf'):\n",
    "        svm = make_pipeline(SMOTE(sampling_strategy = strategy), \\\n",
    "                            SVC(cache_size=4096, kernel='%s' % i, probability = True, class_weight = 'balanced'))\n",
    "        scores = cross_validate(svm, x_train, y_train, scoring = scoring, cv = 10, n_jobs=10)\n",
    "        temp_metrix = calcu_metrix(scores)\n",
    "        print(i)\n",
    "        print(scores)\n",
    "        if temp_metrix > best_metrix:\n",
    "            best = svm\n",
    "            best_scores = scores\n",
    "        svm.fit(x_train, y_train)\n",
    "        scoreList.append((i, svm.score(x_test, y_test)))\n",
    "        \n",
    "    print(\"Maximum kernel Score is {:.2f}%\".format((max([x[1] for x in scoreList])) * 100))\n",
    "    \n",
    "    best_scores = pd.DataFrame(best_scores).transpose()\n",
    "    best_scores.to_csv('./Prediction_output_%s/svm_cross_validation_score.csv' % title, index=True)\n",
    "    print_report('svm', best, x_test, y_test)\n",
    "\n",
    "    return best\n",
    "\n",
    "def svm_im(x_train, y_train,x_test, y_test):\n",
    "\n",
    "    best = SVC(cache_size=4096, kernel='linear')\n",
    "    best_metrix = 0\n",
    "    \n",
    "    # try to find the best kernel\n",
    "    scoreList = []\n",
    "    for i in ('poly', 'rbf'):\n",
    "        svm = SVC(cache_size=4096, kernel='%s' % i, probability = True)\n",
    "        scores = cross_validate(svm, x_train, y_train, scoring = scoring, cv = 10, n_jobs=10)\n",
    "        temp_metrix = calcu_metrix(scores)\n",
    "        print(i)\n",
    "        print(scores)\n",
    "        if temp_metrix > best_metrix:\n",
    "            best = svm\n",
    "            best_scores = scores\n",
    "        svm.fit(x_train, y_train)\n",
    "        scoreList.append((i, svm.score(x_test, y_test)))\n",
    "\n",
    "\n",
    "    plt.title('Various kernels of SVM algorithms')\n",
    "    plt.plot([x[0] for x in scoreList], [x[1] for x in scoreList])\n",
    "    plt.xticks([x[0] for x in scoreList])\n",
    "    plt.xlabel(\"Kernel\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(\"./Prediction_output_%s/svm_im_kernel.png\" % title)\n",
    "    plt.show()\n",
    "    print(\"Maximum kernel Score is {:.2f}%\".format((max([x[1] for x in scoreList])) * 100))\n",
    "\n",
    "    best_scores = pd.DataFrame(best_scores).transpose()\n",
    "    best_scores.to_csv('./Prediction_output_%s/svm_im_cross_validation_score.csv' % title, index=True)\n",
    "    print_report('svm_im', best, x_test, y_test)\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(x_train, y_train, x_test, y_test, strategy, class_weight = None):\n",
    "    \n",
    "    #使用网格搜索 查找最优参数\n",
    "    \n",
    "#     param_grid = [{'randomforestclassifier__n_estimators': [2000, 1000, 500],\n",
    "#                'randomforestclassifier__max_features': [50, 30],\n",
    "#                'randomforestclassifier__max_depth': [20, 15, 10]}]\n",
    "    \n",
    "    param_grid = [{'randomforestclassifier__n_estimators': [1000],\n",
    "                   'randomforestclassifier__max_depth': [20]}]\n",
    "\n",
    "    best = make_pipeline(SMOTE(sampling_strategy = strategy), \\\n",
    "                         RandomForestClassifier(n_jobs=20, n_estimators = 1000, \\\n",
    "                                                                    max_depth = 20, class_weight = class_weight))\n",
    "#     grid_search = GridSearchCV(best, param_grid, cv=5, n_jobs=-1)\n",
    "#     grid_search.fit(x_train, y_train)\n",
    "\n",
    "#     best_scores = cross_validate(best, x_train, y_train, scoring = scoring, cv = 10, n_jobs=-1)\n",
    "    \n",
    "#     print(scores)\n",
    "    best.fit(x_train, y_train)\n",
    "    print(\"Test score: {:.2f}\".format(best.score(x_test, y_test)))\n",
    "    \n",
    "#     best_scores = pd.DataFrame(best_scores).transpose()\n",
    "#     best_scores.to_csv('./Prediction_output/svm_im_cross_validation_score.csv', index=True)\n",
    "    print_report('random_forest', best, x_test, y_test)\n",
    "\n",
    "    return best\n",
    "\n",
    "def random_forest_im(x_train, y_train, x_test, y_test, class_weight = None):\n",
    "\n",
    "    #使用网格搜索 查找最优参数\n",
    "    \n",
    "#     param_grid = [{'randomforestclassifier__n_estimators': [2000, 1000, 500],\n",
    "#                'randomforestclassifier__max_features': [50, 30],\n",
    "#                'randomforestclassifier__max_depth': [20, 15, 10]}]\n",
    "    \n",
    "    best_tested_param_grid = [{'randomforestclassifier__n_estimators': [1000],\n",
    "                   'randomforestclassifier__max_depth': [20]}]\n",
    "\n",
    "    best = RandomForestClassifier(n_jobs=20, n_estimators = 1000, max_depth = 20, class_weight = class_weight)\n",
    "    best.fit(x_train, y_train)\n",
    "#     grid_search = GridSearchCV(best, param_grid, cv=5, n_jobs=-1)\n",
    "#     grid_search.fit(x_train, y_train)\n",
    "\n",
    "#     scores = cross_validate(best, x_train, y_train, scoring = scoring, cv = 10, n_jobs=-1)\n",
    "    \n",
    "#     print(scores)\n",
    "#     print(\"parameters: \", grid_search.best_params_)\n",
    "    print(\"Test score: {:.2f}\".format(best.score(x_test, y_test)))\n",
    "    \n",
    "#     best_scores = pd.DataFrame(best_scores).transpose()\n",
    "#     best_scores.to_csv('./Prediction_output/random_forest_im_cross_validation_score.csv', index=True)\n",
    "    print_report('random_forest_im', best, x_test, y_test)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x_train, y_train, x_test, y_test, strategy):\n",
    "    mlp = make_pipeline(SMOTE(random_state = 1, sampling_strategy = strategy), \\\n",
    "                        MLPClassifier(hidden_layer_sizes=(80, 80, 80), max_iter=200, alpha=0.0001,\n",
    "                                                          solver='adam',verbose= 10, tol=0.000000001))\n",
    "    mlp.fit(x_train, y_train)\n",
    "    print(\"Accuracy of mlp model is {:.3f}\".format(mlp.score(x_test, y_test)))\n",
    "    proba = mlp.predict_proba(x_test)\n",
    "    # log_proba = mlp.predict_log_proba(x_test)\n",
    "    print(mlp.predict(x_test))\n",
    "    pred_mlp = mlp.predict(x_test)\n",
    "    print((pred_mlp == y_test).sum() / y_test.size)\n",
    "    \n",
    "    print_report('mlp', mlp, x_test, y_test)\n",
    "    return mlp\n",
    "\n",
    "def mlp_im(x_train, y_train, x_test, y_test):\n",
    "    mlp_im = MLPClassifier(hidden_layer_sizes=(80, 80, 80), max_iter=200, alpha=0.0001,\n",
    "                     solver='adam', verbose=10, tol=0.000000001)\n",
    "    mlp_im.fit(x_train, y_train)\n",
    "    print(\"Accuracy of mlp_im imbalance model is {:.3f}\".format(mlp_im.score(x_test, y_test)))\n",
    "    proba = mlp_im.predict_proba(x_test)\n",
    "    # log_proba = mlp_im.predict_log_proba(x_test)\n",
    "    print(mlp_im.predict(x_test))\n",
    "    pred_mlp_im = mlp_im.predict(x_test)\n",
    "    print((pred_mlp_im == y_test).sum() / y_test.size)\n",
    "\n",
    "    print_report('mlp_im', mlp_im, x_test, y_test)\n",
    "    return mlp_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(x_train, y_train, x_test, y_test, strategy):\n",
    "    nb = make_pipeline(SMOTE(random_state = 5, sampling_strategy = strategy), GaussianNB(var_smoothing=1e-16))\n",
    "    nb.fit(x_train, y_train)\n",
    "#     scores = cross_validate(nb, x_train, y_train, scoring = scoring, cv = 10)\n",
    "#     print(scores)\n",
    "    print(\"Accuracy of Naive Bayes: {:.2f}%\".format(nb.score(x_test,y_test)*100))\n",
    "\n",
    "    print_report('nb', nb, x_test, y_test)\n",
    "    \n",
    "#     scores = pd.DataFrame(scores).transpose()\n",
    "#     scores.to_csv('./Prediction_output/nb_cross_validation_score.csv', index=True)\n",
    "    \n",
    "    return nb\n",
    "\n",
    "def naive_bayes_im(x_train, y_train, x_test, y_test):\n",
    "    nb = GaussianNB(var_smoothing=1e-16)\n",
    "    nb.fit(x_train, y_train)\n",
    "#     scores = cross_validate(nb, x_train, y_train, scoring = scoring, cv = 10)\n",
    "#     print(scores)\n",
    "    print(\"Accuracy of Naive Bayes: {:.2f}%\".format(nb.score(x_test, y_test)*100))\n",
    "\n",
    "    print_report('nb_im', nb, x_test, y_test)\n",
    "#     scores = pd.DataFrame(scores).transpose()\n",
    "#     scores.to_csv('./Prediction_output/nb_cross_validation_score.csv', index=True)\n",
    "    \n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting(x_train, y_train, x_test, y_test, strategy):\n",
    "\n",
    "\n",
    "    # 网格搜索算法查询最优参数（n_estimators 树个数；learning_rate 学习率； max_depth 树深度 ）\n",
    "    # params_gbrt = [{\n",
    "    #     'gradientboostingclassifier__n_estimators':[500,1000,2000],\n",
    "    #     'gradientboostingclassifier__learning_rate':[0.01,0.1,0.05],\n",
    "    #     'gradientboostingclassifier__max_depth':[7, 9, 11]\n",
    "    # }]\n",
    "\n",
    "    params_gbrt = {\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.05,\n",
    "        'max_depth': 15\n",
    "    }\n",
    "\n",
    "    # gbrt_grid = GridSearchCV(gbrt, params_gbrt, cv=5, n_jobs=10)\n",
    "    # gbrt_grid.fit(x_train, y_train)\n",
    "    # #gbrt.fit(x_train, y_train)\n",
    "\n",
    "    # print('Report of gradient_boosting:\\n')\n",
    "    # print(\"Best cross-validation score: {:.2f}\".format(gbrt_grid.best_score_))\n",
    "    # print(\"Best parameters: \", gbrt_grid.best_params_)\n",
    "    # print(\"Accuracy on training set:{:.3f}\".format(gbrt_grid.score(x_train, y_train)))\n",
    "    # print(\"Accuracy on test set:{:.3f}\".format(gbrt_grid.score(x_test, y_test)))\n",
    "\n",
    "    ## 使用gbrt的最优参数构建模型 并评估模型性能\n",
    "    # best_params_gbrt = gbrt_grid.best_params_\n",
    "\n",
    "    gbrt_model = make_pipeline(SMOTE(random_state = 1, sampling_strategy = dict), \\\n",
    "                              GradientBoostingClassifier(**params_gbrt))\n",
    "    gbrt_model.fit(x_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on test set:{:.3f}\".format(gbrt_model.score(x_test, y_test)))\n",
    "    print_report('gb', gbrt_model, x_test, y_test)\n",
    "\n",
    "    return gbrt_model\n",
    "\n",
    "def gradient_boosting_im(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    params_gbrt = {\n",
    "        'n_estimators':1000,\n",
    "        'learning_rate':0.05,\n",
    "        'max_depth': 15\n",
    "    }\n",
    "\n",
    "#     gbrt_grid = GridSearchCV(gbrt, params_gbrt, cv=5, n_jobs=5)\n",
    "#     gbrt_grid.fit(x_train, y_train)\n",
    "    #gbrt.fit(x_train, y_train)\n",
    "\n",
    "#     print('Report of gradient_boosting:\\n')\n",
    "#     print(\"Best cross-validation score: {:.2f}\".format(gbrt_grid.best_score_))\n",
    "#     print(\"Best parameters: \", gbrt_grid.best_params_)\n",
    "#     print(\"Accuracy on training set:{:.3f}\".format(gbrt_grid.score(x_train, y_train)))\n",
    "#     print(\"Accuracy on test set:{:.3f}\".format(gbrt_grid.score(x_test, y_test)))\n",
    "\n",
    "#     ## 使用gbrt的最优参数构建模型 并评估模型性能\n",
    "#     best_params_gbrt = gbrt_grid.best_params_\n",
    "\n",
    "    gbrt_model = GradientBoostingClassifier(**params_gbrt)\n",
    "    gbrt_model.fit(x_train, y_train)\n",
    "    \n",
    "#     scores = cross_validate(gbrt_model, x_train, y_train, scoring = scoring, cv = 10, n_jobs=10)\n",
    "    print(\"Accuracy on train set:{:.3f}\".format(gbrt_model.score(x_test, y_test)))\n",
    "    print(\"Accuracy on test set:{:.3f}\".format(gbrt_model.score(x_test, y_test)))\n",
    "\n",
    "    print_report('gb_im', gbrt_model, x_test, y_test)\n",
    "\n",
    "    return gbrt_model    # 梯度提升回归树  GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of family RF00050: 7.48%\n",
      "Proportion of family RF00059: 22.34%\n",
      "Proportion of family RF00162: 7.67%\n",
      "Proportion of family RF00167: 5.02%\n",
      "Proportion of family RF00168: 2.11%\n",
      "Proportion of family RF00174: 26.48%\n",
      "Proportion of family RF00234: 1.70%\n",
      "Proportion of family RF00380: 1.34%\n",
      "Proportion of family RF00504: 13.13%\n",
      "Proportion of family RF00521: 1.03%\n",
      "Proportion of family RF00522: 0.72%\n",
      "Proportion of family RF00634: 1.33%\n",
      "Proportion of family RF01051: 6.30%\n",
      "Proportion of family RF01054: 0.24%\n",
      "Proportion of family RF01055: 1.87%\n",
      "Proportion of family RF01057: 1.22%\n",
      "{'RF00050': 827, 'RF00059': 2551, 'RF00162': 852, 'RF00167': 587, 'RF00168': 245, 'RF00174': 2993, 'RF00234': 181, 'RF00380': 152, 'RF00504': 1491, 'RF00521': 133, 'RF00522': 81, 'RF00634': 155, 'RF01051': 701, 'RF01054': 21, 'RF01055': 210, 'RF01057': 148}\n",
      "{'RF00050': 1654, 'RF00059': 2551, 'RF00162': 1704, 'RF00167': 1761, 'RF00168': 1470, 'RF00174': 2993, 'RF00234': 1810, 'RF00380': 1520, 'RF00504': 1491, 'RF00521': 1330, 'RF00522': 810, 'RF00634': 1550, 'RF01051': 1402, 'RF01054': 210, 'RF01055': 1050, 'RF01057': 1480}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    countpath = './156_new_selected_156.csv'\n",
    "\n",
    "    x_train, y_train, x_test, y_test, featurenames, label, dataframe = preprocess(countpath, 0.3, 3)\n",
    "    \n",
    "    \n",
    "    mkdir('./Prediction_output_%s/Classification_report' % title)\n",
    "    count = counting(label, y_train)\n",
    "    dict = smote_dict(label, count)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "----------------------------------------------------------------\n",
      "6\n",
      "{'fit_time': array([4.43994713, 4.78112125, 4.9448688 , 4.68320704, 4.62062192,\n",
      "       4.58889556, 4.61679506, 4.52378368, 4.62616992, 4.44306064]), 'score_time': array([54.84095383, 52.52633715, 48.94019985, 55.06800056, 53.2062552 ,\n",
      "       55.56206155, 48.42211056, 53.33631349, 53.04119802, 55.47709036]), 'test_acc': array([0.90201225, 0.89094107, 0.88722467, 0.89673433, 0.88526037,\n",
      "       0.8938992 , 0.88770999, 0.88928255, 0.8998227 , 0.90248227]), 'test_prec_macro': array([0.77892734, 0.81275415, 0.78803293, 0.79025638, 0.76280762,\n",
      "       0.76166472, 0.77832317, 0.75858734, 0.79454169, 0.80219922]), 'test_rec_macro': array([0.88318647, 0.91865683, 0.90108608, 0.916483  , 0.88578968,\n",
      "       0.90229232, 0.91011123, 0.88785084, 0.92617285, 0.93118056]), 'test_f1_macro': array([0.81666056, 0.85438993, 0.83218976, 0.8389666 , 0.80872004,\n",
      "       0.81019102, 0.82848147, 0.80377286, 0.84242567, 0.85308986])}\n",
      "8\n",
      "{'fit_time': array([4.4726584 , 4.4856503 , 4.50707626, 4.35712814, 5.13992214,\n",
      "       4.47111678, 4.99719405, 4.50122261, 5.7429893 , 4.72106099]), 'score_time': array([48.05324054, 48.36191535, 52.51131701, 51.53767395, 52.84471941,\n",
      "       52.81757617, 52.93479824, 52.90001488, 52.49423027, 50.92040324]), 'test_acc': array([0.90551181, 0.88302551, 0.88105727, 0.89143866, 0.88614298,\n",
      "       0.8974359 , 0.88240495, 0.88662533, 0.8891844 , 0.89539007]), 'test_prec_macro': array([0.77655266, 0.79931655, 0.77814259, 0.77773071, 0.75925245,\n",
      "       0.76312855, 0.76193746, 0.74360182, 0.77132879, 0.78662399]), 'test_rec_macro': array([0.89431646, 0.91976106, 0.90275145, 0.91572441, 0.88931804,\n",
      "       0.90703839, 0.90861055, 0.89028397, 0.92165244, 0.92874472]), 'test_f1_macro': array([0.82155976, 0.84501886, 0.82525135, 0.8281491 , 0.80719729,\n",
      "       0.81269152, 0.81437254, 0.79626992, 0.82462668, 0.83950925])}\n",
      "10\n",
      "{'fit_time': array([5.97513294, 4.48165298, 4.02649307, 5.78626704, 5.2541945 ,\n",
      "       5.37134719, 4.41859746, 5.83130312, 4.12016368, 5.01534581]), 'score_time': array([60.16543555, 61.04421234, 62.065979  , 63.293957  , 61.33422256,\n",
      "       57.15181279, 61.45190907, 59.26578975, 54.38705683, 62.89001918]), 'test_acc': array([0.89151356, 0.87686895, 0.8784141 , 0.88526037, 0.87908208,\n",
      "       0.89124668, 0.87533156, 0.88308237, 0.8856383 , 0.89184397]), 'test_prec_macro': array([0.7571654 , 0.7630441 , 0.7757202 , 0.76838744, 0.75274076,\n",
      "       0.75996361, 0.75261557, 0.73616013, 0.76381993, 0.78036798]), 'test_rec_macro': array([0.88467117, 0.91500208, 0.90489752, 0.9170105 , 0.89049711,\n",
      "       0.91511901, 0.90623487, 0.8876644 , 0.92334282, 0.93438803]), 'test_f1_macro': array([0.8032603 , 0.82030298, 0.82355104, 0.82182851, 0.80190906,\n",
      "       0.81237259, 0.80685898, 0.79000001, 0.81945434, 0.83654797])}\n",
      "12\n",
      "{'fit_time': array([5.24918127, 5.53141499, 5.31275225, 5.20969176, 5.69423127,\n",
      "       5.09658551, 5.04767656, 4.87704873, 5.05933452, 5.21359301]), 'score_time': array([62.32595944, 62.624547  , 60.33426118, 58.68158793, 60.34289646,\n",
      "       62.85990167, 63.17072964, 58.51016021, 62.28388548, 60.4871676 ]), 'test_acc': array([0.88626422, 0.8707124 , 0.87400881, 0.88526037, 0.87643425,\n",
      "       0.88505747, 0.86914235, 0.87953942, 0.88386525, 0.88741135]), 'test_prec_macro': array([0.7440926 , 0.75938073, 0.77291317, 0.7617827 , 0.74847282,\n",
      "       0.74989875, 0.7393601 , 0.73105957, 0.7583912 , 0.76829334]), 'test_rec_macro': array([0.87455866, 0.91805363, 0.90662954, 0.91677293, 0.88987157,\n",
      "       0.91498069, 0.90865716, 0.88959544, 0.92431041, 0.93799572]), 'test_f1_macro': array([0.7904261 , 0.81769065, 0.82143376, 0.81412464, 0.79728458,\n",
      "       0.80371051, 0.79834836, 0.78617124, 0.81616535, 0.82726939])}\n",
      "14\n",
      "{'fit_time': array([4.29343534, 4.49458075, 4.44741011, 4.47489238, 4.3380909 ,\n",
      "       4.31620145, 4.06461549, 4.1982336 , 4.29137945, 4.33733273]), 'score_time': array([58.70640469, 57.2459116 , 58.27761841, 58.32061148, 54.99061704,\n",
      "       54.93481231, 58.27420664, 56.26619792, 58.67070818, 51.58051896]), 'test_acc': array([0.88363955, 0.86631486, 0.86696035, 0.87908208, 0.87378641,\n",
      "       0.8841733 , 0.87179487, 0.86979628, 0.87677305, 0.8785461 ]), 'test_prec_macro': array([0.73886099, 0.74201015, 0.76397107, 0.75103029, 0.74072593,\n",
      "       0.74749991, 0.73825602, 0.71761283, 0.74055025, 0.75959665]), 'test_rec_macro': array([0.87683824, 0.91637211, 0.9068311 , 0.91937796, 0.88905176,\n",
      "       0.91469952, 0.90877634, 0.88128545, 0.92101365, 0.93461599]), 'test_f1_macro': array([0.78677644, 0.80327831, 0.81426088, 0.80689795, 0.79217996,\n",
      "       0.80127009, 0.79793164, 0.77371095, 0.8007919 , 0.8214018 ])}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum KNN Acuracy Score is 90.42%\n",
      "f1 score of knn: 0.856\n",
      "Classification report of knn: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.849     0.953     0.898       384\n",
      "     RF00059      0.986     0.944     0.964      1065\n",
      "     RF00162      0.882     0.943     0.912       389\n",
      "     RF00167      0.829     0.881     0.854       226\n",
      "     RF00168      0.484     0.802     0.604        96\n",
      "     RF00174      0.992     0.814     0.894      1293\n",
      "     RF00234      0.618     0.862     0.720        94\n",
      "     RF00380      0.596     0.954     0.734        65\n",
      "     RF00504      0.987     0.937     0.961       634\n",
      "     RF00521      0.919     1.000     0.958        34\n",
      "     RF00522      0.972     1.000     0.986        35\n",
      "     RF00634      0.759     0.984     0.857        61\n",
      "     RF01051      0.957     0.987     0.972       319\n",
      "     RF01054      0.720     1.000     0.837        18\n",
      "     RF01055      0.647     0.828     0.726        93\n",
      "     RF01057      0.716     0.960     0.821        50\n",
      "\n",
      "    accuracy                          0.904      4856\n",
      "   macro avg      0.807     0.928     0.856      4856\n",
      "weighted avg      0.923     0.904     0.909      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('knn')\n",
    "print('----------------------------------------------------------------')\n",
    "knn = KNN(x_train, y_train, x_test, y_test,'not majority')\n",
    "save(knn, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_im\n",
      "----------------------------------------------------------------\n",
      "6\n",
      "{'fit_time': array([0.2709744 , 0.12616825, 0.16589332, 0.1597085 , 0.27166176,\n",
      "       0.1643424 , 0.27999187, 0.14076781, 0.14701438, 0.14739299]), 'score_time': array([18.20588279, 18.06362319, 18.4332068 , 16.95914745, 16.91894531,\n",
      "       16.71238685, 18.34007239, 15.81618762, 16.51496434, 17.34400582]), 'test_acc': array([0.92388451, 0.92172383, 0.92422907, 0.92850838, 0.92497793,\n",
      "       0.93810787, 0.92661362, 0.914969  , 0.93085106, 0.94148936]), 'test_prec_macro': array([0.84473568, 0.91067008, 0.91484784, 0.91872257, 0.85807856,\n",
      "       0.86942158, 0.86542127, 0.88985317, 0.92591954, 0.88804023]), 'test_rec_macro': array([0.81551748, 0.83969018, 0.84481853, 0.87742724, 0.8098496 ,\n",
      "       0.80791442, 0.81500838, 0.80383561, 0.87099401, 0.8167626 ]), 'test_f1_macro': array([0.82651434, 0.86416255, 0.86840475, 0.89312472, 0.82741994,\n",
      "       0.82894035, 0.83354233, 0.82628707, 0.89165649, 0.84211783])}\n",
      "8\n",
      "{'fit_time': array([0.19924259, 0.1428318 , 0.13897395, 0.19500446, 0.20784831,\n",
      "       0.18852091, 0.1674633 , 0.14315915, 0.13880801, 0.13680172]), 'score_time': array([22.11662316, 17.94101262, 14.48319411, 24.49480438, 18.92509079,\n",
      "       19.17368698, 20.31382942, 20.40941429, 22.48993659, 21.08601975]), 'test_acc': array([0.9256343 , 0.92612137, 0.92070485, 0.93292145, 0.92321271,\n",
      "       0.93457118, 0.92572944, 0.90788308, 0.93262411, 0.94148936]), 'test_prec_macro': array([0.84771006, 0.9177869 , 0.84665467, 0.92716709, 0.92385038,\n",
      "       0.8593229 , 0.91882918, 0.81740938, 0.92717161, 0.93968646]), 'test_rec_macro': array([0.8182069 , 0.84700485, 0.80920727, 0.87818155, 0.82542118,\n",
      "       0.8030674 , 0.86413597, 0.76391805, 0.87970143, 0.84882521]), 'test_f1_macro': array([0.82961771, 0.87299842, 0.82188544, 0.89782378, 0.86044696,\n",
      "       0.82341927, 0.88435844, 0.77255059, 0.89844886, 0.88033244])}\n",
      "10\n",
      "{'fit_time': array([0.19625044, 0.1852057 , 0.26134968, 0.17258191, 0.20482945,\n",
      "       0.24742866, 0.2261281 , 0.20649505, 0.23075891, 0.2375226 ]), 'score_time': array([21.06556273, 22.13555932, 25.17832589, 25.55300832, 36.57092214,\n",
      "       25.0556066 , 24.91541767, 27.50967121, 25.01399422, 25.22988176]), 'test_acc': array([0.92300962, 0.92084433, 0.91189427, 0.93027361, 0.91791703,\n",
      "       0.93545535, 0.92219275, 0.91231178, 0.92819149, 0.93794326]), 'test_prec_macro': array([0.83081344, 0.8985062 , 0.83757938, 0.92498147, 0.85573542,\n",
      "       0.86322347, 0.90319983, 0.83700065, 0.91928895, 0.94348819]), 'test_rec_macro': array([0.81082165, 0.84520675, 0.80337503, 0.8683568 , 0.79010575,\n",
      "       0.79805852, 0.83606248, 0.77101319, 0.83764149, 0.83425115]), 'test_f1_macro': array([0.81815017, 0.86416323, 0.81432165, 0.88924799, 0.81417684,\n",
      "       0.82032777, 0.85949226, 0.78382669, 0.86800119, 0.8700932 ])}\n",
      "12\n",
      "{'fit_time': array([0.15404272, 0.15373826, 0.13782382, 0.15048528, 0.15421653,\n",
      "       0.15112424, 0.14449573, 0.1520319 , 0.19805527, 0.13613629]), 'score_time': array([17.46177459, 13.90389442, 17.2314775 , 17.39150858, 14.79977822,\n",
      "       16.08159757, 17.24131322, 17.68335032, 15.59155154, 16.5567081 ]), 'test_acc': array([0.92125984, 0.92612137, 0.90837004, 0.92497793, 0.91879965,\n",
      "       0.93015031, 0.91600354, 0.90876882, 0.92730496, 0.93617021]), 'test_prec_macro': array([0.83759741, 0.90396581, 0.83620924, 0.91783571, 0.86092515,\n",
      "       0.85801825, 0.84645837, 0.82951256, 0.92499059, 0.94523553]), 'test_rec_macro': array([0.79578745, 0.85748446, 0.78898127, 0.83323694, 0.79136228,\n",
      "       0.7917878 , 0.79638095, 0.76627942, 0.83787679, 0.83327993]), 'test_f1_macro': array([0.81116981, 0.87364216, 0.80580999, 0.86213311, 0.81811523,\n",
      "       0.81609218, 0.8145548 , 0.77944815, 0.87005528, 0.86983635])}\n",
      "14\n",
      "{'fit_time': array([0.17461371, 0.17369461, 0.15631461, 0.17190671, 0.16389632,\n",
      "       0.1713388 , 0.16778684, 0.35153627, 0.34592199, 0.14836717]), 'score_time': array([34.74257159, 20.25377822, 20.54462504, 23.4581387 , 20.67690492,\n",
      "       27.3563385 , 20.92404652, 34.84294367, 27.95116806, 27.86818624]), 'test_acc': array([0.91513561, 0.91380827, 0.91629956, 0.9223301 , 0.91350397,\n",
      "       0.92484527, 0.91600354, 0.90699734, 0.92109929, 0.93262411]), 'test_prec_macro': array([0.83693752, 0.82993493, 0.84891468, 0.91396647, 0.85556474,\n",
      "       0.84459684, 0.8421812 , 0.83648605, 0.91393925, 0.94562625]), 'test_rec_macro': array([0.78264138, 0.81394429, 0.79844879, 0.8334196 , 0.7822828 ,\n",
      "       0.77395529, 0.78987735, 0.76460056, 0.82027143, 0.82529805]), 'test_f1_macro': array([0.80275353, 0.81939159, 0.81596086, 0.86252151, 0.80904968,\n",
      "       0.80043059, 0.80895604, 0.77632392, 0.85409263, 0.86485783])}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum KNN_im Acuracy Score is 92.61%\n",
      "f1 score of knn_im: 0.848\n",
      "Classification report of knn_im: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.903     0.943     0.922       384\n",
      "     RF00059      0.933     0.983     0.957      1065\n",
      "     RF00162      0.888     0.941     0.914       389\n",
      "     RF00167      0.829     0.858     0.843       226\n",
      "     RF00168      0.775     0.573     0.659        96\n",
      "     RF00174      0.938     0.940     0.939      1293\n",
      "     RF00234      0.859     0.585     0.696        94\n",
      "     RF00380      0.806     0.831     0.818        65\n",
      "     RF00504      0.962     0.948     0.955       634\n",
      "     RF00521      0.941     0.941     0.941        34\n",
      "     RF00522      0.946     1.000     0.972        35\n",
      "     RF00634      0.902     0.902     0.902        61\n",
      "     RF01051      0.953     0.947     0.950       319\n",
      "     RF01054      1.000     0.333     0.500        18\n",
      "     RF01055      0.919     0.613     0.735        93\n",
      "     RF01057      0.894     0.840     0.866        50\n",
      "\n",
      "    accuracy                          0.922      4856\n",
      "   macro avg      0.903     0.824     0.848      4856\n",
      "weighted avg      0.922     0.922     0.920      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('knn_im')\n",
    "print('----------------------------------------------------------------')\n",
    "knn_im = KNN_im(x_train, y_train, x_test, y_test)\n",
    "save(knn_im, 'knn_im')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "----------------------------------------------------------------\n",
      "poly\n",
      "{'fit_time': array([642.27426791, 652.04786468, 621.11930418, 638.08146596,\n",
      "       630.23782349, 645.51847196, 722.93994546, 655.06188917,\n",
      "       642.29706812, 677.10336065]), 'score_time': array([17.57484412, 17.99469352, 20.07663941, 18.33270669, 20.02912831,\n",
      "       17.54042912,  3.78045535, 17.79686999, 21.96779585, 20.01897287]), 'test_acc': array([0.96937883, 0.96745822, 0.96475771, 0.96822595, 0.97616946,\n",
      "       0.96816976, 0.97259063, 0.96102746, 0.97429078, 0.97251773]), 'test_prec_macro': array([0.94489283, 0.94528599, 0.92612005, 0.95763842, 0.95399888,\n",
      "       0.94504219, 0.96353216, 0.92605608, 0.96328822, 0.94986504]), 'test_rec_macro': array([0.92756421, 0.95111763, 0.95026397, 0.9573681 , 0.94279884,\n",
      "       0.93841358, 0.95513375, 0.93852314, 0.94243055, 0.95916469]), 'test_f1_macro': array([0.93197647, 0.94651797, 0.93720699, 0.95419156, 0.94237525,\n",
      "       0.94059927, 0.95820379, 0.93112476, 0.95127612, 0.95274931])}\n",
      "rbf\n",
      "{'fit_time': array([1999.47315931, 1532.98330212, 1504.3457911 , 1519.27849174,\n",
      "       1523.64923334, 1533.55928564, 1532.69590092, 1552.19615483,\n",
      "       1548.37282848, 1538.38173246]), 'score_time': array([30.66417193, 30.19289541, 36.84164858, 35.13337517, 32.99558687,\n",
      "       29.96280789, 30.42356944, 27.97834253, 27.03344655, 27.90281463]), 'test_acc': array([0.9720035 , 0.97449428, 0.969163  , 0.97705207, 0.97616946,\n",
      "       0.97966401, 0.97789567, 0.97254207, 0.97606383, 0.97074468]), 'test_prec_macro': array([0.8937901 , 0.97174095, 0.96361584, 0.9728375 , 0.96172299,\n",
      "       0.96151603, 0.97234402, 0.95727718, 0.97338373, 0.96926363]), 'test_rec_macro': array([0.89462882, 0.95993149, 0.93322194, 0.96172652, 0.91526941,\n",
      "       0.92952048, 0.952369  , 0.95668976, 0.94598899, 0.93137091]), 'test_f1_macro': array([0.89306089, 0.9651016 , 0.94722535, 0.96668165, 0.93143512,\n",
      "       0.94013533, 0.96149867, 0.95634336, 0.95835908, 0.94822421])}\n",
      "Maximum kernel Score is 97.92%\n",
      "f1 score of svm: 0.968\n",
      "Classification report of svm: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.997     0.961     0.979       384\n",
      "     RF00059      0.992     0.993     0.993      1065\n",
      "     RF00162      0.995     0.985     0.990       389\n",
      "     RF00167      0.925     0.925     0.925       226\n",
      "     RF00168      0.817     0.792     0.804        96\n",
      "     RF00174      0.971     0.991     0.981      1293\n",
      "     RF00234      0.976     0.872     0.921        94\n",
      "     RF00380      0.970     0.985     0.977        65\n",
      "     RF00504      0.994     0.995     0.994       634\n",
      "     RF00521      1.000     1.000     1.000        34\n",
      "     RF00522      0.972     1.000     0.986        35\n",
      "     RF00634      1.000     0.984     0.992        61\n",
      "     RF01051      0.981     0.991     0.986       319\n",
      "     RF01054      1.000     1.000     1.000        18\n",
      "     RF01055      0.978     0.946     0.962        93\n",
      "     RF01057      0.980     1.000     0.990        50\n",
      "\n",
      "    accuracy                          0.979      4856\n",
      "   macro avg      0.972     0.964     0.968      4856\n",
      "weighted avg      0.979     0.979     0.979      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('svm')\n",
    "print('----------------------------------------------------------------')\n",
    "svm = svm(x_train, y_train, x_test, y_test, 'not majority')\n",
    "save(svm, 'svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_im\n",
      "----------------------------------------------------------------\n",
      "poly\n",
      "{'fit_time': array([120.56179404, 125.20292783, 123.80568314, 123.13318038,\n",
      "       119.1363852 , 119.7047894 , 123.69413352, 122.30617094,\n",
      "       121.00287843, 120.85168004]), 'score_time': array([2.31619382, 2.68065572, 2.62282681, 2.53460383, 2.28441644,\n",
      "       2.27244902, 2.57486153, 2.45555878, 2.33087659, 2.38074851]), 'test_acc': array([0.9720035 , 0.97273527, 0.969163  , 0.97087379, 0.98058252,\n",
      "       0.96993811, 0.9734748 , 0.96368468, 0.97429078, 0.97251773]), 'test_prec_macro': array([0.94650157, 0.95018337, 0.93561525, 0.95948595, 0.95831792,\n",
      "       0.95114727, 0.96522667, 0.93111813, 0.95887257, 0.95301863]), 'test_rec_macro': array([0.93743263, 0.95998477, 0.95391279, 0.96319664, 0.94765116,\n",
      "       0.94294512, 0.95534278, 0.94057145, 0.94569374, 0.96034813]), 'test_f1_macro': array([0.93876309, 0.95399365, 0.94395273, 0.95860256, 0.94723283,\n",
      "       0.94557795, 0.95924342, 0.93498049, 0.95078247, 0.95563261])}\n",
      "rbf\n",
      "{'fit_time': array([377.69099712, 373.71974897, 378.90568495, 378.11909866,\n",
      "       380.05980086, 380.5959239 , 392.07432365, 383.78615475,\n",
      "       396.50003529, 394.70067263]), 'score_time': array([24.89008927, 23.15050983, 22.82148457, 22.6187253 , 24.54161143,\n",
      "       24.2533505 , 21.44530034, 23.32662773, 24.02258658, 20.68776441]), 'test_acc': array([0.96500437, 0.97361478, 0.96387665, 0.9717564 , 0.96910856,\n",
      "       0.9734748 , 0.97170645, 0.96722764, 0.97429078, 0.96985816]), 'test_prec_macro': array([0.88819861, 0.97254396, 0.96001121, 0.96996727, 0.96161352,\n",
      "       0.89616494, 0.96242188, 0.9585764 , 0.97336586, 0.96785105]), 'test_rec_macro': array([0.87559186, 0.95977465, 0.91788184, 0.94442837, 0.89252061,\n",
      "       0.87569389, 0.91282998, 0.93733084, 0.93734436, 0.89756685]), 'test_f1_macro': array([0.88112424, 0.965715  , 0.93647848, 0.95631383, 0.91837703,\n",
      "       0.88521181, 0.93146412, 0.94602582, 0.95422067, 0.92498336])}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum kernel Score is 97.53%\n",
      "f1 score of svm_im: 0.955\n",
      "Classification report of svm_im: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.995     0.958     0.976       384\n",
      "     RF00059      0.991     0.993     0.992      1065\n",
      "     RF00162      0.992     0.982     0.987       389\n",
      "     RF00167      0.932     0.907     0.919       226\n",
      "     RF00168      0.794     0.802     0.798        96\n",
      "     RF00174      0.962     0.992     0.977      1293\n",
      "     RF00234      0.988     0.840     0.908        94\n",
      "     RF00380      0.968     0.923     0.945        65\n",
      "     RF00504      0.995     0.995     0.995       634\n",
      "     RF00521      1.000     0.971     0.985        34\n",
      "     RF00522      0.972     1.000     0.986        35\n",
      "     RF00634      0.967     0.967     0.967        61\n",
      "     RF01051      0.975     0.991     0.983       319\n",
      "     RF01054      1.000     0.889     0.941        18\n",
      "     RF01055      0.977     0.925     0.950        93\n",
      "     RF01057      0.980     0.960     0.970        50\n",
      "\n",
      "    accuracy                          0.975      4856\n",
      "   macro avg      0.968     0.943     0.955      4856\n",
      "weighted avg      0.975     0.975     0.975      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('svm_im')\n",
    "print('----------------------------------------------------------------')\n",
    "svm_im = svm_im(x_train, y_train, x_test, y_test)\n",
    "save(svm_im, 'svm_im')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "----------------------------------------------------------------\n",
      "Test score: 0.98\n",
      "f1 score of random_forest: 0.974\n",
      "Classification report of random_forest: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.997     0.977     0.987       384\n",
      "     RF00059      0.993     0.998     0.996      1065\n",
      "     RF00162      0.982     0.985     0.983       389\n",
      "     RF00167      0.935     0.889     0.912       226\n",
      "     RF00168      0.761     0.865     0.810        96\n",
      "     RF00174      0.990     0.994     0.992      1293\n",
      "     RF00234      1.000     1.000     1.000        94\n",
      "     RF00380      0.985     0.985     0.985        65\n",
      "     RF00504      0.995     0.994     0.994       634\n",
      "     RF00521      1.000     1.000     1.000        34\n",
      "     RF00522      0.972     1.000     0.986        35\n",
      "     RF00634      0.984     0.984     0.984        61\n",
      "     RF01051      0.991     0.997     0.994       319\n",
      "     RF01054      1.000     1.000     1.000        18\n",
      "     RF01055      1.000     0.925     0.961        93\n",
      "     RF01057      1.000     1.000     1.000        50\n",
      "\n",
      "    accuracy                          0.984      4856\n",
      "   macro avg      0.974     0.974     0.974      4856\n",
      "weighted avg      0.985     0.984     0.984      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('random forest')\n",
    "print('----------------------------------------------------------------')\n",
    "random = random_forest(x_train, y_train, x_test, y_test, dict)\n",
    "save(random, 'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest_im\n",
      "----------------------------------------------------------------\n",
      "Test score: 0.98\n",
      "f1 score of random_forest_im: 0.945\n",
      "Classification report of random_forest_im: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.989     0.979     0.984       384\n",
      "     RF00059      0.991     0.998     0.994      1065\n",
      "     RF00162      0.970     0.982     0.976       389\n",
      "     RF00167      0.907     0.912     0.909       226\n",
      "     RF00168      0.796     0.771     0.783        96\n",
      "     RF00174      0.981     0.995     0.988      1293\n",
      "     RF00234      1.000     0.957     0.978        94\n",
      "     RF00380      0.982     0.862     0.918        65\n",
      "     RF00504      0.989     0.995     0.992       634\n",
      "     RF00521      1.000     1.000     1.000        34\n",
      "     RF00522      1.000     1.000     1.000        35\n",
      "     RF00634      1.000     0.951     0.975        61\n",
      "     RF01051      0.981     0.997     0.989       319\n",
      "     RF01054      1.000     0.556     0.714        18\n",
      "     RF01055      0.988     0.871     0.926        93\n",
      "     RF01057      1.000     0.980     0.990        50\n",
      "\n",
      "    accuracy                          0.978      4856\n",
      "   macro avg      0.973     0.925     0.945      4856\n",
      "weighted avg      0.978     0.978     0.978      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('random_forest_im')\n",
    "print('----------------------------------------------------------------')\n",
    "random_im = random_forest_im(x_train, y_train, x_test, y_test)\n",
    "save(random_im, 'random_im')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:0.958\n",
      "f1 score of gb: 0.904\n",
      "Classification report of gb: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.986     0.932     0.959       384\n",
      "     RF00059      0.987     0.993     0.990      1065\n",
      "     RF00162      0.867     0.956     0.910       389\n",
      "     RF00167      0.881     0.885     0.883       226\n",
      "     RF00168      0.739     0.708     0.723        96\n",
      "     RF00174      0.967     0.987     0.977      1293\n",
      "     RF00234      0.979     0.979     0.979        94\n",
      "     RF00380      0.869     0.815     0.841        65\n",
      "     RF00504      0.990     0.983     0.987       634\n",
      "     RF00521      0.861     0.912     0.886        34\n",
      "     RF00522      1.000     0.943     0.971        35\n",
      "     RF00634      0.897     0.852     0.874        61\n",
      "     RF01051      0.972     0.972     0.972       319\n",
      "     RF01054      1.000     0.556     0.714        18\n",
      "     RF01055      1.000     0.731     0.845        93\n",
      "     RF01057      1.000     0.920     0.958        50\n",
      "\n",
      "    accuracy                          0.958      4856\n",
      "   macro avg      0.937     0.883     0.904      4856\n",
      "weighted avg      0.958     0.958     0.957      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = gradient_boosting(x_train, y_train, x_test, y_test, dict)\n",
    "save(gb, 'gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:0.952\n",
      "Accuracy on test set:0.952\n",
      "f1 score of gb_im: 0.892\n",
      "Classification report of gb_im: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.992     0.930     0.960       384\n",
      "     RF00059      0.982     0.997     0.990      1065\n",
      "     RF00162      0.984     0.949     0.966       389\n",
      "     RF00167      0.895     0.872     0.883       226\n",
      "     RF00168      0.784     0.604     0.682        96\n",
      "     RF00174      0.960     0.994     0.976      1293\n",
      "     RF00234      0.989     0.968     0.978        94\n",
      "     RF00380      0.902     0.569     0.698        65\n",
      "     RF00504      0.880     0.994     0.933       634\n",
      "     RF00521      0.966     0.824     0.889        34\n",
      "     RF00522      0.967     0.829     0.892        35\n",
      "     RF00634      0.980     0.803     0.883        61\n",
      "     RF01051      0.968     0.944     0.956       319\n",
      "     RF01054      0.727     0.889     0.800        18\n",
      "     RF01055      0.971     0.731     0.834        93\n",
      "     RF01057      1.000     0.920     0.958        50\n",
      "\n",
      "    accuracy                          0.952      4856\n",
      "   macro avg      0.934     0.863     0.892      4856\n",
      "weighted avg      0.953     0.952     0.950      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_im = gradient_boosting_im(x_train, y_train, x_test, y_test)\n",
    "save(gb_im, 'gb_im')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.00522025\n",
      "Iteration 14, loss = 0.00388803\n",
      "Iteration 15, loss = 0.00278992\n",
      "Iteration 16, loss = 0.00146109\n",
      "Iteration 17, loss = 0.00115895\n",
      "Iteration 18, loss = 0.00106251\n",
      "Iteration 19, loss = 0.00246342\n",
      "Iteration 20, loss = 0.01460089\n",
      "Iteration 21, loss = 0.00331203\n",
      "Iteration 22, loss = 0.00097880\n",
      "Iteration 23, loss = 0.00056183\n",
      "Iteration 24, loss = 0.00045352\n",
      "Iteration 25, loss = 0.00040291\n",
      "Iteration 26, loss = 0.00036746\n",
      "Iteration 27, loss = 0.00032568\n",
      "Iteration 28, loss = 0.00030229\n",
      "Iteration 29, loss = 0.00027801\n",
      "Iteration 30, loss = 0.00026383\n",
      "Iteration 31, loss = 0.00024465\n",
      "Iteration 32, loss = 0.00023237\n",
      "Iteration 33, loss = 0.00021995\n",
      "Iteration 34, loss = 0.00021005\n",
      "Iteration 35, loss = 0.00020012\n",
      "Iteration 36, loss = 0.00019460\n",
      "Iteration 37, loss = 0.00018810\n",
      "Iteration 38, loss = 0.00018144\n",
      "Iteration 39, loss = 0.00017416\n",
      "Iteration 40, loss = 0.00016910\n",
      "Iteration 41, loss = 0.00016618\n",
      "Iteration 42, loss = 0.00016074\n",
      "Iteration 43, loss = 0.00015807\n",
      "Iteration 44, loss = 0.00015481\n",
      "Iteration 45, loss = 0.00015244\n",
      "Iteration 46, loss = 0.00014902\n",
      "Iteration 47, loss = 0.00014681\n",
      "Iteration 48, loss = 0.00014439\n",
      "Iteration 49, loss = 0.00014231\n",
      "Iteration 50, loss = 0.00014135\n",
      "Iteration 51, loss = 0.00013894\n",
      "Iteration 52, loss = 0.00013736\n",
      "Iteration 53, loss = 0.00013619\n",
      "Iteration 54, loss = 0.00013487\n",
      "Iteration 55, loss = 0.00013360\n",
      "Iteration 56, loss = 0.00013239\n",
      "Iteration 57, loss = 0.00013133\n",
      "Iteration 58, loss = 0.00013057\n",
      "Iteration 59, loss = 0.00012930\n",
      "Iteration 60, loss = 0.00012868\n",
      "Iteration 61, loss = 0.00012783\n",
      "Iteration 62, loss = 0.00012686\n",
      "Iteration 63, loss = 0.00012611\n",
      "Iteration 64, loss = 0.00012553\n",
      "Iteration 65, loss = 0.00012476\n",
      "Iteration 66, loss = 0.00012405\n",
      "Iteration 67, loss = 0.00012348\n",
      "Iteration 68, loss = 0.00012276\n",
      "Iteration 69, loss = 0.00012237\n",
      "Iteration 70, loss = 0.00012177\n",
      "Iteration 71, loss = 0.00012114\n",
      "Iteration 72, loss = 0.00012048\n",
      "Iteration 73, loss = 0.00012000\n",
      "Iteration 74, loss = 0.00011934\n",
      "Iteration 75, loss = 0.00011885\n",
      "Iteration 76, loss = 0.00011826\n",
      "Iteration 77, loss = 0.00011766\n",
      "Iteration 78, loss = 0.00011709\n",
      "Iteration 79, loss = 0.00011655\n",
      "Iteration 80, loss = 0.00011604\n",
      "Iteration 81, loss = 0.00011536\n",
      "Iteration 82, loss = 0.00011478\n",
      "Iteration 83, loss = 0.00011430\n",
      "Iteration 84, loss = 0.00011352\n",
      "Iteration 85, loss = 0.00011292\n",
      "Iteration 86, loss = 0.00011231\n",
      "Iteration 87, loss = 0.00011161\n",
      "Iteration 88, loss = 0.00011086\n",
      "Iteration 89, loss = 0.00011058\n",
      "Iteration 90, loss = 0.00010979\n",
      "Iteration 91, loss = 0.00010888\n",
      "Iteration 92, loss = 0.07167364\n",
      "Iteration 93, loss = 0.00649246\n",
      "Iteration 94, loss = 0.00127279\n",
      "Iteration 95, loss = 0.00065497\n",
      "Iteration 96, loss = 0.00047321\n",
      "Iteration 97, loss = 0.00037654\n",
      "Iteration 98, loss = 0.00032811\n",
      "Iteration 99, loss = 0.00029101\n",
      "Iteration 100, loss = 0.00026334\n",
      "Iteration 101, loss = 0.00024313\n",
      "Iteration 102, loss = 0.00022626\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Accuracy of mlp model is 0.985\n",
      "['RF01051' 'RF00504' 'RF00504' ... 'RF00059' 'RF01051' 'RF00162']\n",
      "0.9853789126853377\n",
      "f1 score of mlp: 0.970\n",
      "Classification report of mlp: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.995     0.974     0.984       384\n",
      "     RF00059      0.997     1.000     0.999      1065\n",
      "     RF00162      0.984     0.977     0.981       389\n",
      "     RF00167      0.921     0.929     0.925       226\n",
      "     RF00168      0.842     0.833     0.838        96\n",
      "     RF00174      0.993     0.995     0.994      1293\n",
      "     RF00234      1.000     0.979     0.989        94\n",
      "     RF00380      0.941     0.985     0.962        65\n",
      "     RF00504      0.992     0.997     0.994       634\n",
      "     RF00521      0.971     1.000     0.986        34\n",
      "     RF00522      1.000     1.000     1.000        35\n",
      "     RF00634      0.984     1.000     0.992        61\n",
      "     RF01051      0.994     0.994     0.994       319\n",
      "     RF01054      1.000     0.889     0.941        18\n",
      "     RF01055      0.957     0.946     0.951        93\n",
      "     RF01057      0.980     1.000     0.990        50\n",
      "\n",
      "    accuracy                          0.985      4856\n",
      "   macro avg      0.972     0.969     0.970      4856\n",
      "weighted avg      0.985     0.985     0.985      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = mlp(x_train, y_train, x_test, y_test, dict)\n",
    "save(mlp, 'mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.47405874\n",
      "Iteration 2, loss = 0.37864528\n",
      "Iteration 3, loss = 0.17034583\n",
      "Iteration 4, loss = 0.09903850\n",
      "Iteration 5, loss = 0.07487691\n",
      "Iteration 6, loss = 0.05819773\n",
      "Iteration 7, loss = 0.04633099\n",
      "Iteration 8, loss = 0.03873211\n",
      "Iteration 9, loss = 0.03509532\n",
      "Iteration 10, loss = 0.02887242\n",
      "Iteration 11, loss = 0.02533878\n",
      "Iteration 12, loss = 0.02202773\n",
      "Iteration 13, loss = 0.01988704\n",
      "Iteration 14, loss = 0.01804913\n",
      "Iteration 15, loss = 0.02028243\n",
      "Iteration 16, loss = 0.01651955\n",
      "Iteration 17, loss = 0.01608278\n",
      "Iteration 18, loss = 0.01372465\n",
      "Iteration 19, loss = 0.01288617\n",
      "Iteration 20, loss = 0.01099862\n",
      "Iteration 21, loss = 0.00917824\n",
      "Iteration 22, loss = 0.00877432\n",
      "Iteration 23, loss = 0.00807995\n",
      "Iteration 24, loss = 0.00715767\n",
      "Iteration 25, loss = 0.00989943\n",
      "Iteration 26, loss = 0.00862901\n",
      "Iteration 27, loss = 0.00455140\n",
      "Iteration 28, loss = 0.00380310\n",
      "Iteration 29, loss = 0.00312410\n",
      "Iteration 30, loss = 0.00329690\n",
      "Iteration 31, loss = 0.00261298\n",
      "Iteration 32, loss = 0.00338711\n",
      "Iteration 33, loss = 0.00248180\n",
      "Iteration 34, loss = 0.00227659\n",
      "Iteration 35, loss = 0.00294985\n",
      "Iteration 36, loss = 0.00133409\n",
      "Iteration 37, loss = 0.00112145\n",
      "Iteration 38, loss = 0.00130461\n",
      "Iteration 39, loss = 0.00091231\n",
      "Iteration 40, loss = 0.00078634\n",
      "Iteration 41, loss = 0.00070081\n",
      "Iteration 42, loss = 0.00062959\n",
      "Iteration 43, loss = 0.00061160\n",
      "Iteration 44, loss = 0.00055332\n",
      "Iteration 45, loss = 0.00052741\n",
      "Iteration 46, loss = 0.00049580\n",
      "Iteration 47, loss = 0.00046530\n",
      "Iteration 48, loss = 0.00045451\n",
      "Iteration 49, loss = 0.00041742\n",
      "Iteration 50, loss = 0.00042149\n",
      "Iteration 51, loss = 0.00037461\n",
      "Iteration 52, loss = 0.00035874\n",
      "Iteration 53, loss = 0.00034815\n",
      "Iteration 54, loss = 0.00036757\n",
      "Iteration 55, loss = 0.00033264\n",
      "Iteration 56, loss = 0.00030531\n",
      "Iteration 57, loss = 0.00029332\n",
      "Iteration 58, loss = 0.00028336\n",
      "Iteration 59, loss = 0.00027879\n",
      "Iteration 60, loss = 0.00027130\n",
      "Iteration 61, loss = 0.00025847\n",
      "Iteration 62, loss = 0.00025510\n",
      "Iteration 63, loss = 0.00024477\n",
      "Iteration 64, loss = 0.00023802\n",
      "Iteration 65, loss = 0.00022993\n",
      "Iteration 66, loss = 0.00022520\n",
      "Iteration 67, loss = 0.00022049\n",
      "Iteration 68, loss = 0.00021683\n",
      "Iteration 69, loss = 0.00021254\n",
      "Iteration 70, loss = 0.00020548\n",
      "Iteration 71, loss = 0.00020221\n",
      "Iteration 72, loss = 0.00019767\n",
      "Iteration 73, loss = 0.00019258\n",
      "Iteration 74, loss = 0.00019238\n",
      "Iteration 75, loss = 0.00018588\n",
      "Iteration 76, loss = 0.00018362\n",
      "Iteration 77, loss = 0.00018105\n",
      "Iteration 78, loss = 0.00017832\n",
      "Iteration 79, loss = 0.00017537\n",
      "Iteration 80, loss = 0.00017313\n",
      "Iteration 81, loss = 0.00017075\n",
      "Iteration 82, loss = 0.00016820\n",
      "Iteration 83, loss = 0.00016699\n",
      "Iteration 84, loss = 0.00016475\n",
      "Iteration 85, loss = 0.00016264\n",
      "Iteration 86, loss = 0.00016070\n",
      "Iteration 87, loss = 0.00015917\n",
      "Iteration 88, loss = 0.00015809\n",
      "Iteration 89, loss = 0.00015619\n",
      "Iteration 90, loss = 0.00015495\n",
      "Iteration 91, loss = 0.00015348\n",
      "Iteration 92, loss = 0.00015184\n",
      "Iteration 93, loss = 0.00015201\n",
      "Iteration 94, loss = 0.00014958\n",
      "Iteration 95, loss = 0.00014801\n",
      "Iteration 96, loss = 0.00014786\n",
      "Iteration 97, loss = 0.00014626\n",
      "Iteration 98, loss = 0.00014540\n",
      "Iteration 99, loss = 0.00014463\n",
      "Iteration 100, loss = 0.00014306\n",
      "Iteration 101, loss = 0.00014265\n",
      "Iteration 102, loss = 0.00014141\n",
      "Iteration 103, loss = 0.00014068\n",
      "Iteration 104, loss = 0.00014023\n",
      "Iteration 105, loss = 0.00013907\n",
      "Iteration 106, loss = 0.00013843\n",
      "Iteration 107, loss = 0.00013827\n",
      "Iteration 108, loss = 0.00013703\n",
      "Iteration 109, loss = 0.00013697\n",
      "Iteration 110, loss = 0.00013585\n",
      "Iteration 111, loss = 0.00013559\n",
      "Iteration 112, loss = 0.00013487\n",
      "Iteration 113, loss = 0.00013460\n",
      "Iteration 114, loss = 0.00013392\n",
      "Iteration 115, loss = 0.00013334\n",
      "Iteration 116, loss = 0.00013289\n",
      "Iteration 117, loss = 0.00013244\n",
      "Iteration 118, loss = 0.00013202\n",
      "Iteration 119, loss = 0.00013170\n",
      "Iteration 120, loss = 0.00013164\n",
      "Iteration 121, loss = 0.00013075\n",
      "Iteration 122, loss = 0.00013027\n",
      "Iteration 123, loss = 0.00012997\n",
      "Iteration 124, loss = 0.00012963\n",
      "Iteration 125, loss = 0.00012925\n",
      "Iteration 126, loss = 0.00012882\n",
      "Iteration 127, loss = 0.00012845\n",
      "Iteration 128, loss = 0.00012802\n",
      "Iteration 129, loss = 0.00012784\n",
      "Iteration 130, loss = 0.00012824\n",
      "Iteration 131, loss = 0.00012725\n",
      "Iteration 132, loss = 0.00012678\n",
      "Iteration 133, loss = 0.00012641\n",
      "Iteration 134, loss = 0.00012600\n",
      "Iteration 135, loss = 0.00012593\n",
      "Iteration 136, loss = 0.00012550\n",
      "Iteration 137, loss = 0.00012538\n",
      "Iteration 138, loss = 0.00012489\n",
      "Iteration 139, loss = 0.00012477\n",
      "Iteration 140, loss = 0.00012442\n",
      "Iteration 141, loss = 0.00012409\n",
      "Iteration 142, loss = 0.00012374\n",
      "Iteration 143, loss = 0.00012345\n",
      "Iteration 144, loss = 0.00012343\n",
      "Iteration 145, loss = 0.00012281\n",
      "Iteration 146, loss = 0.00012262\n",
      "Iteration 147, loss = 0.00012258\n",
      "Iteration 148, loss = 0.00012199\n",
      "Iteration 149, loss = 0.00012182\n",
      "Iteration 150, loss = 0.00012171\n",
      "Iteration 151, loss = 0.00012141\n",
      "Iteration 152, loss = 0.00012096\n",
      "Iteration 153, loss = 0.00012076\n",
      "Iteration 154, loss = 0.00012055\n",
      "Iteration 155, loss = 0.00012019\n",
      "Iteration 156, loss = 0.00011999\n",
      "Iteration 157, loss = 0.00011979\n",
      "Iteration 158, loss = 0.00011936\n",
      "Iteration 159, loss = 0.00011912\n",
      "Iteration 160, loss = 0.00011894\n",
      "Iteration 161, loss = 0.00011862\n",
      "Iteration 162, loss = 0.00011828\n",
      "Iteration 163, loss = 0.00011796\n",
      "Iteration 164, loss = 0.00011782\n",
      "Iteration 165, loss = 0.00011737\n",
      "Iteration 166, loss = 0.00011723\n",
      "Iteration 167, loss = 0.00011695\n",
      "Iteration 168, loss = 0.00011670\n",
      "Iteration 169, loss = 0.00011628\n",
      "Iteration 170, loss = 0.00011597\n",
      "Iteration 171, loss = 0.00011571\n",
      "Iteration 172, loss = 0.00011535\n",
      "Iteration 173, loss = 0.00011496\n",
      "Iteration 174, loss = 0.00011468\n",
      "Iteration 175, loss = 0.00011434\n",
      "Iteration 176, loss = 0.03430310\n",
      "Iteration 177, loss = 0.26822257\n",
      "Iteration 178, loss = 0.01221521\n",
      "Iteration 179, loss = 0.00607195\n",
      "Iteration 180, loss = 0.00447861\n",
      "Iteration 181, loss = 0.00310313\n",
      "Iteration 182, loss = 0.00261851\n",
      "Iteration 183, loss = 0.00214714\n",
      "Iteration 184, loss = 0.00134125\n",
      "Iteration 185, loss = 0.00120394\n",
      "Iteration 186, loss = 0.00098573\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Accuracy of mlp_im imbalance model is 0.982\n",
      "['RF01051' 'RF00504' 'RF00504' ... 'RF00059' 'RF01051' 'RF00162']\n",
      "0.982495881383855\n",
      "f1 score of mlp_im: 0.961\n",
      "Classification report of mlp_im: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.995     0.971     0.983       384\n",
      "     RF00059      0.995     0.998     0.997      1065\n",
      "     RF00162      0.985     0.985     0.985       389\n",
      "     RF00167      0.890     0.929     0.909       226\n",
      "     RF00168      0.805     0.729     0.765        96\n",
      "     RF00174      0.992     0.993     0.992      1293\n",
      "     RF00234      0.989     0.989     0.989        94\n",
      "     RF00380      0.955     0.969     0.962        65\n",
      "     RF00504      0.995     1.000     0.998       634\n",
      "     RF00521      0.971     0.971     0.971        34\n",
      "     RF00522      0.972     1.000     0.986        35\n",
      "     RF00634      0.968     0.984     0.976        61\n",
      "     RF01051      0.997     0.991     0.994       319\n",
      "     RF01054      1.000     0.889     0.941        18\n",
      "     RF01055      0.967     0.957     0.962        93\n",
      "     RF01057      0.942     0.980     0.961        50\n",
      "\n",
      "    accuracy                          0.982      4856\n",
      "   macro avg      0.964     0.958     0.961      4856\n",
      "weighted avg      0.982     0.982     0.982      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_im = mlp_im(x_train, y_train, x_test, y_test)\n",
    "save(mlp_im, 'mlp_im')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report('mlp_im_test', mlp_im, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb\n",
      "-------------------------------------------------------\n",
      "Accuracy of Naive Bayes: 89.64%\n",
      "f1 score of nb: 0.771\n",
      "Classification report of nb: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.980     0.896     0.936       384\n",
      "     RF00059      0.993     0.958     0.975      1065\n",
      "     RF00162      0.968     0.856     0.909       389\n",
      "     RF00167      0.893     0.668     0.765       226\n",
      "     RF00168      0.478     0.896     0.623        96\n",
      "     RF00174      0.926     0.924     0.925      1293\n",
      "     RF00234      0.851     0.915     0.882        94\n",
      "     RF00380      0.563     0.892     0.690        65\n",
      "     RF00504      0.988     0.909     0.947       634\n",
      "     RF00521      0.313     0.912     0.466        34\n",
      "     RF00522      0.742     0.657     0.697        35\n",
      "     RF00634      0.495     0.885     0.635        61\n",
      "     RF01051      0.989     0.834     0.905       319\n",
      "     RF01054      0.455     0.556     0.500        18\n",
      "     RF01055      0.760     0.817     0.788        93\n",
      "     RF01057      0.571     0.880     0.693        50\n",
      "\n",
      "    accuracy                          0.896      4856\n",
      "   macro avg      0.748     0.841     0.771      4856\n",
      "weighted avg      0.924     0.896     0.905      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('nb')\n",
    "print('-------------------------------------------------------')\n",
    "nb = naive_bayes(x_train, y_train, x_test, y_test, dict)\n",
    "save(nb, 'nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_im\n",
      "-------------------------------------------------------\n",
      "Accuracy of Naive Bayes: 84.43%\n",
      "f1 score of nb_im: 0.705\n",
      "Classification report of nb_im: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050      0.985     0.870     0.924       384\n",
      "     RF00059      1.000     0.935     0.967      1065\n",
      "     RF00162      0.976     0.743     0.844       389\n",
      "     RF00167      0.943     0.664     0.779       226\n",
      "     RF00168      0.455     0.833     0.588        96\n",
      "     RF00174      0.962     0.866     0.912      1293\n",
      "     RF00234      0.712     0.894     0.792        94\n",
      "     RF00380      0.453     0.969     0.618        65\n",
      "     RF00504      0.996     0.825     0.903       634\n",
      "     RF00521      0.136     0.912     0.237        34\n",
      "     RF00522      0.719     0.657     0.687        35\n",
      "     RF00634      0.244     0.885     0.383        61\n",
      "     RF01051      0.987     0.690     0.812       319\n",
      "     RF01054      0.455     0.556     0.500        18\n",
      "     RF01055      0.782     0.849     0.814        93\n",
      "     RF01057      0.376     0.880     0.527        50\n",
      "\n",
      "    accuracy                          0.844      4856\n",
      "   macro avg      0.699     0.814     0.705      4856\n",
      "weighted avg      0.929     0.844     0.872      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('nb_im')\n",
    "print('-------------------------------------------------------')\n",
    "nb_im = naive_bayes_im(x_train, y_train, x_test, y_test)\n",
    "save(nb_im, 'nb_im')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three methods comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.36815619, 1.38522267, 1.27516413, 1.46671844, 1.39341235]), 'score_time': array([65.77883387, 66.15830994, 64.95893312, 67.77423406, 66.04027724]), 'test_precision_macro': array([0.45819163, 0.48098308, 0.45682151, 0.47233651, 0.47222897]), 'test_recall_macro': array([0.69405591, 0.69335485, 0.67254681, 0.7193012 , 0.69062105]), 'test_f1_macro': array([0.46983322, 0.4947362 , 0.46601983, 0.49063489, 0.4851479 ])}\n",
      "f1 score of best: 0.50\n",
      "Classification report of best: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050       0.62      0.79      0.69       359\n",
      "     RF00059       0.95      0.43      0.59      1083\n",
      "     RF00162       0.44      0.80      0.57       365\n",
      "     RF00167       0.54      0.73      0.62       244\n",
      "     RF00168       0.14      0.66      0.23        88\n",
      "     RF00174       1.00      0.33      0.49      1277\n",
      "     RF00234       0.25      0.71      0.37        90\n",
      "     RF00380       0.20      0.79      0.32        63\n",
      "     RF00504       0.86      0.56      0.68       661\n",
      "     RF00521       0.15      0.64      0.25        55\n",
      "     RF00522       0.57      0.88      0.69        41\n",
      "     RF00634       0.33      0.86      0.48        63\n",
      "     RF01051       0.62      0.65      0.64       292\n",
      "     RF01054       0.19      0.92      0.32        13\n",
      "     RF01055       0.51      0.82      0.63        94\n",
      "     RF01057       0.32      0.84      0.46        68\n",
      "\n",
      "    accuracy                           0.54      4856\n",
      "   macro avg       0.48      0.71      0.50      4856\n",
      "weighted avg       0.77      0.54      0.57      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best = make_pipeline(SMOTE(random_state = 5), KNeighborsClassifier(n_neighbors=10))\n",
    "scores = cross_validate(best, x_train, y_train, scoring = scoring, cv = 5, n_jobs=10)\n",
    "print(scores)\n",
    "best.fit(x_train, y_train)\n",
    "best.score(x_test, y_test)\n",
    "best_pred = best.predict(x_test)\n",
    "# f1 score\n",
    "print(\"f1 score of best: {:.2f}\".format(f1_score(y_test,best_pred, average = 'macro')))\n",
    "# 模型评估报告\n",
    "print(\"Classification report of best: \\n{}\".format(classification_report(y_test, best_pred,\n",
    "                                        target_names= label)))\n",
    "best_proba = best.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.04368973, 0.04368138, 0.04063821, 0.04307175, 0.04468369]), 'score_time': array([11.63832211, 11.69619298, 12.09875536, 11.89288187, 11.96761632]), 'test_precision_macro': array([0.76222982, 0.77800996, 0.71748333, 0.7782801 , 0.79469287]), 'test_recall_macro': array([0.51248768, 0.5246983 , 0.50299728, 0.52320519, 0.49453976]), 'test_f1_macro': array([0.5736022 , 0.57565019, 0.55515465, 0.5859372 , 0.55578544])}\n",
      "f1 score of best3: 0.60\n",
      "Classification report of best2: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050       0.83      0.78      0.81       359\n",
      "     RF00059       0.79      0.92      0.85      1083\n",
      "     RF00162       0.65      0.74      0.69       365\n",
      "     RF00167       0.86      0.59      0.70       244\n",
      "     RF00168       0.57      0.48      0.52        88\n",
      "     RF00174       0.88      0.87      0.88      1277\n",
      "     RF00234       0.80      0.27      0.40        90\n",
      "     RF00380       0.79      0.48      0.59        63\n",
      "     RF00504       0.69      0.90      0.78       661\n",
      "     RF00521       0.67      0.04      0.07        55\n",
      "     RF00522       1.00      0.61      0.76        41\n",
      "     RF00634       0.88      0.35      0.50        63\n",
      "     RF01051       0.75      0.73      0.74       292\n",
      "     RF01054       1.00      0.08      0.14        13\n",
      "     RF01055       0.93      0.53      0.68        94\n",
      "     RF01057       0.88      0.32      0.47        68\n",
      "\n",
      "    accuracy                           0.79      4856\n",
      "   macro avg       0.81      0.54      0.60      4856\n",
      "weighted avg       0.80      0.79      0.78      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best2 = KNeighborsClassifier(n_neighbors=16)\n",
    "scores2 = cross_validate(best2, x_train, y_train, scoring = scoring, cv = 5, n_jobs=10)\n",
    "print(scores2)\n",
    "best2.fit(x_train, y_train)\n",
    "best2.score(x_test, y_test)\n",
    "best2_pred = best2.predict(x_test)\n",
    "# f1 score\n",
    "print(\"f1 score of best3: {:.2f}\".format(f1_score(y_test,best2_pred, average = 'macro')))\n",
    "# 模型评估报告\n",
    "print(\"Classification report of best2: \\n{}\".format(classification_report(y_test, best2_pred,\n",
    "                                        target_names= label)))\n",
    "best2_proba = best2.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.22312522, 0.29851818, 0.29342937, 0.18135238, 0.28857732]), 'score_time': array([168.14797163, 165.46073842, 166.19052601, 162.7279706 ,\n",
      "       163.33171201]), 'test_precision_macro': array([0.95304545, 0.96063604, 0.95937588, 0.9568318 , 0.95625031]), 'test_recall_macro': array([0.91273627, 0.92393875, 0.91939974, 0.90468659, 0.90314451]), 'test_f1_macro': array([0.92442428, 0.93549652, 0.93110554, 0.91621002, 0.91123336])}\n",
      "f1 score of best3: 0.46\n",
      "Classification report of best3: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     RF00050       0.55      0.79      0.65       359\n",
      "     RF00059       0.98      0.26      0.41      1083\n",
      "     RF00162       0.40      0.80      0.53       365\n",
      "     RF00167       0.50      0.72      0.59       244\n",
      "     RF00168       0.11      0.66      0.19        88\n",
      "     RF00174       1.00      0.17      0.30      1277\n",
      "     RF00234       0.24      0.73      0.36        90\n",
      "     RF00380       0.17      0.78      0.28        63\n",
      "     RF00504       0.86      0.55      0.67       661\n",
      "     RF00521       0.15      0.65      0.24        55\n",
      "     RF00522       0.57      0.88      0.69        41\n",
      "     RF00634       0.31      0.86      0.45        63\n",
      "     RF01051       0.58      0.65      0.61       292\n",
      "     RF01054       0.16      0.92      0.28        13\n",
      "     RF01055       0.45      0.82      0.58        94\n",
      "     RF01057       0.30      0.84      0.45        68\n",
      "\n",
      "    accuracy                           0.46      4856\n",
      "   macro avg       0.46      0.69      0.46      4856\n",
      "weighted avg       0.76      0.46      0.46      4856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "over_samples_train = SMOTEENN(random_state=5, n_jobs=5)\n",
    "x_train_s, y_train_s = over_samples_train.fit_sample(x_train, y_train)\n",
    "best3 = KNeighborsClassifier(n_neighbors=16)\n",
    "scores3 = cross_validate(best3, x_train_s, y_train_s, scoring = scoring, cv = 5, n_jobs=10)\n",
    "print(scores3)\n",
    "best3.fit(x_train_s, y_train_s)\n",
    "best3.score(x_test, y_test)\n",
    "\n",
    "best3_pred = best3.predict(x_test)\n",
    "# f1 score\n",
    "print(\"f1 score of best3: {:.2f}\".format(f1_score(y_test,best3_pred, average = 'macro')))\n",
    "# 模型评估报告\n",
    "print(\"Classification report of best3: \\n{}\".format(classification_report(y_test, best3_pred,\n",
    "                                        target_names= label)))\n",
    "best3_proba = best3.predict_proba(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
